# ä¼˜åŒ–çš„A/Bæ± è½®æ¢ç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬

## ğŸ“‹ éœ€æ±‚ä¿®æ­£å’Œå…³é”®çº¦æŸ

### ç”¨æˆ·åé¦ˆä¿®æ­£
- âœ… **æ‰¹é‡è·å–é™åˆ¶**: æœ€å¤š200ä¸ªä»£ç†ï¼Œæµ‹è¯•æœŸé—´ä½¿ç”¨20ä¸ª
- âœ… **7åˆ†é’Ÿè½®æ¢é—´éš”**: A/Bæ± æ™ºèƒ½åˆ‡æ¢
- âœ… **10åˆ†é’ŸIPç”Ÿå‘½å‘¨æœŸ**: ä»£ç†IPæœ‰æ•ˆæœŸé™åˆ¶
- âœ… **æ•°æ®åº“URLè¯»å–**: ä»æ•°æ®åº“ç›´æ¥è·å–ä»£ç†URL

---

## ğŸ”§ ä¿®æ­£çš„æ‰¹é‡è·å–ç­–ç•¥

### æ‰¹é‡é…ç½®å‚æ•° (ä¿®æ­£ç‰ˆ)
```python
# æ‰¹é‡è·å–é…ç½® - éµå¾ªç”¨æˆ·é™åˆ¶
PROXY_BATCH_CONFIGS = {
    "production": {
        "batch_size": 200,        # ç”Ÿäº§ç¯å¢ƒæ‰¹é‡å¤§å° (æ¥å£ä¸Šé™)
        "min_threshold": 100,     # æœ€å°è§¦å‘é˜ˆå€¼
        "prefetch_buffer": 80,    # é¢„å–ç¼“å†²åŒºå¤§å°
        "max_concurrent": 2       # æœ€å¤§å¹¶å‘æ‰¹æ¬¡
    },
    "development": {
        "batch_size": 20,         # å¼€å‘/æµ‹è¯•ç¯å¢ƒæ‰¹é‡å¤§å°
        "min_threshold": 10,      # æœ€å°è§¦å‘é˜ˆå€¼
        "prefetch_buffer": 5,     # é¢„å–ç¼“å†²åŒºå¤§å°
        "max_concurrent": 1       # æœ€å¤§å¹¶å‘æ‰¹æ¬¡
    }
}

# è½®æ¢æ—¶é—´é…ç½®
ROTATION_CONFIG = {
    "rotation_interval": 420,     # 7åˆ†é’Ÿ = 420ç§’
    "ip_lifetime": 600,          # 10åˆ†é’Ÿ = 600ç§’
    "overlap_window": 180,       # 3åˆ†é’Ÿé‡å æœŸ
    "warmup_duration": 120       # 2åˆ†é’Ÿé¢„çƒ­æ—¶é—´
}
```

### ä¼˜åŒ–çš„A/Bæ± Repository (ä¿®æ­£ç‰ˆ)

```python
# src/infrastructure/optimized_ab_pool_repository.py
import asyncio
import os
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from enum import Enum

from saturn_mousehunter_shared import get_logger
from .base_repository import BaseRepository

class Environment(Enum):
    DEVELOPMENT = "development"
    PRODUCTION = "production"

@dataclass
class ProxyConfig:
    """ä»£ç†é…ç½®æ•°æ®ç»“æ„"""
    url: str
    type: str
    quality_score: float
    last_used: Optional[datetime] = None
    created_at: datetime = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

    @property
    def age_seconds(self) -> float:
        """è·å–ä»£ç†å¹´é¾„(ç§’)"""
        return (datetime.now() - self.created_at).total_seconds()

    @property
    def is_expired(self) -> bool:
        """æ£€æŸ¥ä»£ç†æ˜¯å¦è¿‡æœŸ(10åˆ†é’Ÿ)"""
        return self.age_seconds > 600  # 10åˆ†é’Ÿ

class OptimizedABPoolRepository(BaseRepository):
    """ä¼˜åŒ–çš„A/Bæ± Repository - ä¿®æ­£æ‰¹é‡é™åˆ¶ç‰ˆæœ¬"""

    def __init__(self, environment: str = "development"):
        super().__init__()
        self.environment = Environment(environment)
        self.config = PROXY_BATCH_CONFIGS[self.environment.value]
        self.logger = get_logger(f"ab_pool_repo_{environment}")

        # A/BåŒæ± å­˜å‚¨
        self.pool_a: List[ProxyConfig] = []
        self.pool_b: List[ProxyConfig] = []
        self.active_pool = "A"

        # æ€§èƒ½ç›‘æ§
        self.stats = {
            "total_fetches": 0,
            "successful_fetches": 0,
            "expired_cleanups": 0,
            "rotation_count": 0,
            "avg_fetch_time": 0.0
        }

        # æ•°æ®åº“è¿æ¥ (ä»ä¾èµ–æ³¨å…¥è·å–)
        self.db_connection = None

    async def initialize(self, db_connection):
        """åˆå§‹åŒ–Repository"""
        self.db_connection = db_connection
        await self._initial_population()
        self.logger.info(f"Initialized optimized AB pool repository",
                        environment=self.environment.value,
                        batch_size=self.config["batch_size"])

    async def _initial_population(self):
        """åˆå§‹åŒ–æ± å¡«å……"""
        # ä¸ºAæ± é¢„å–ä»£ç†
        await self._fetch_proxies_for_pool("A", self.config["batch_size"])

        # ä¸ºBæ± é¢„å–è¾ƒå°‘ä»£ç†ä½œä¸ºé¢„å¤‡
        await self._fetch_proxies_for_pool("B", self.config["prefetch_buffer"])

        self.logger.info("Initial population completed",
                        pool_a_size=len(self.pool_a),
                        pool_b_size=len(self.pool_b))

    async def _fetch_proxies_from_database(self, count: int) -> List[str]:
        """ä»æ•°æ®åº“è·å–ä»£ç†URLåˆ—è¡¨"""
        try:
            # ä¿®æ­£ç‰ˆæœ¬ï¼šä¸¥æ ¼éµå¾ªæ•°é‡é™åˆ¶
            actual_count = min(count, self.config["batch_size"])

            sql = """
            SELECT proxy_url, proxy_type, quality_score
            FROM mh_proxy_pool
            WHERE is_active = true
            AND last_used < NOW() - INTERVAL '5 minutes'
            ORDER BY quality_score DESC, last_used ASC
            LIMIT $1
            """

            rows = await self.db_connection.fetch(sql, actual_count)

            proxy_configs = []
            for row in rows:
                config = ProxyConfig(
                    url=row['proxy_url'],
                    type=row['proxy_type'],
                    quality_score=row['quality_score']
                )
                proxy_configs.append(config)

            self.stats["total_fetches"] += 1
            self.stats["successful_fetches"] += 1

            self.logger.info("Fetched proxies from database",
                           requested_count=count,
                           actual_count=len(proxy_configs),
                           max_allowed=self.config["batch_size"])

            return proxy_configs

        except Exception as e:
            self.logger.error(f"Failed to fetch proxies from database: {e}")
            return []

    async def _fetch_proxies_for_pool(self, pool_name: str, count: int):
        """ä¸ºæŒ‡å®šæ± è·å–ä»£ç†"""
        start_time = datetime.now()

        # ç¡®ä¿ä¸è¶…è¿‡é…ç½®é™åˆ¶
        actual_count = min(count, self.config["batch_size"])

        proxies = await self._fetch_proxies_from_database(actual_count)

        if pool_name == "A":
            self.pool_a.extend(proxies)
        else:
            self.pool_b.extend(proxies)

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        fetch_time = (datetime.now() - start_time).total_seconds()
        self._update_fetch_stats(fetch_time)

        self.logger.info(f"Fetched proxies for pool {pool_name}",
                        count=len(proxies),
                        pool_size=len(self.pool_a if pool_name == "A" else self.pool_b))

    def _update_fetch_stats(self, fetch_time: float):
        """æ›´æ–°è·å–ç»Ÿè®¡"""
        if self.stats["total_fetches"] == 0:
            self.stats["avg_fetch_time"] = fetch_time
        else:
            self.stats["avg_fetch_time"] = (
                self.stats["avg_fetch_time"] * (self.stats["total_fetches"] - 1) + fetch_time
            ) / self.stats["total_fetches"]

    async def get_proxy(self, proxy_type: str = "short") -> Optional[str]:
        """ä»æ´»è·ƒæ± è·å–ä»£ç†"""
        active_pool_list = self.pool_a if self.active_pool == "A" else self.pool_b

        # æ¸…ç†è¿‡æœŸä»£ç†
        await self._cleanup_expired_proxies()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å……ä»£ç†
        if len(active_pool_list) < self.config["min_threshold"]:
            await self._replenish_active_pool()

        # è·å–æœ€ä½³ä»£ç†
        best_proxy = self._select_best_proxy(active_pool_list, proxy_type)

        if best_proxy:
            best_proxy.last_used = datetime.now()
            return best_proxy.url

        return None

    def _select_best_proxy(self, pool: List[ProxyConfig], proxy_type: str) -> Optional[ProxyConfig]:
        """é€‰æ‹©æœ€ä½³ä»£ç†"""
        available_proxies = [p for p in pool if not p.is_expired]

        if not available_proxies:
            return None

        # æŒ‰è´¨é‡åˆ†æ•°å’Œä½¿ç”¨æ—¶é—´æ’åº
        available_proxies.sort(
            key=lambda p: (p.quality_score, -(p.last_used or datetime.min).timestamp()),
            reverse=True
        )

        return available_proxies[0]

    async def _cleanup_expired_proxies(self):
        """æ¸…ç†è¿‡æœŸä»£ç†"""
        before_a = len(self.pool_a)
        before_b = len(self.pool_b)

        self.pool_a = [p for p in self.pool_a if not p.is_expired]
        self.pool_b = [p for p in self.pool_b if not p.is_expired]

        cleaned_count = (before_a + before_b) - (len(self.pool_a) + len(self.pool_b))

        if cleaned_count > 0:
            self.stats["expired_cleanups"] += cleaned_count
            self.logger.info("Cleaned up expired proxies",
                           cleaned_count=cleaned_count,
                           pool_a_size=len(self.pool_a),
                           pool_b_size=len(self.pool_b))

    async def _replenish_active_pool(self):
        """è¡¥å……æ´»è·ƒæ± """
        if self.active_pool == "A":
            needed = self.config["batch_size"] - len(self.pool_a)
        else:
            needed = self.config["batch_size"] - len(self.pool_b)

        if needed > 0:
            # é™åˆ¶è¡¥å……æ•°é‡ä¸è¶…è¿‡é…ç½®é™åˆ¶
            replenish_count = min(needed, self.config["batch_size"])
            await self._fetch_proxies_for_pool(self.active_pool, replenish_count)

    async def perform_rotation(self) -> bool:
        """æ‰§è¡ŒA/Bæ± è½®æ¢"""
        try:
            standby_pool = "B" if self.active_pool == "A" else "A"

            self.logger.info(f"Starting rotation: {self.active_pool} â†’ {standby_pool}")

            # é¢„çƒ­å¾…æœºæ± 
            await self._warmup_standby_pool(standby_pool)

            # æ‰§è¡Œåˆ‡æ¢
            old_active = self.active_pool
            self.active_pool = standby_pool

            # æ›´æ–°ç»Ÿè®¡
            self.stats["rotation_count"] += 1

            self.logger.info("Pool rotation completed",
                           from_pool=old_active,
                           to_pool=self.active_pool,
                           total_rotations=self.stats["rotation_count"])

            # å¼‚æ­¥æ¸…ç†æ—§æ± 
            asyncio.create_task(self._async_cleanup_old_pool(old_active))

            return True

        except Exception as e:
            self.logger.error(f"Pool rotation failed: {e}")
            return False

    async def _warmup_standby_pool(self, pool_name: str):
        """é¢„çƒ­å¾…æœºæ± """
        target_size = self.config["batch_size"]
        current_size = len(self.pool_a if pool_name == "A" else self.pool_b)

        if current_size < target_size:
            needed = target_size - current_size
            await self._fetch_proxies_for_pool(pool_name, needed)

    async def _async_cleanup_old_pool(self, pool_name: str):
        """å¼‚æ­¥æ¸…ç†æ—§æ± åˆ°æœ€å°çŠ¶æ€"""
        await asyncio.sleep(180)  # ç­‰å¾…3åˆ†é’Ÿé‡å æœŸ

        if pool_name == "A":
            self.pool_a = self.pool_a[:self.config["prefetch_buffer"]]
        else:
            self.pool_b = self.pool_b[:self.config["prefetch_buffer"]]

        self.logger.info(f"Cleaned up old pool {pool_name} to standby size")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "environment": self.environment.value,
            "config": self.config,
            "active_pool": self.active_pool,
            "pool_sizes": {
                "pool_a": len(self.pool_a),
                "pool_b": len(self.pool_b)
            },
            "performance": self.stats,
            "health": {
                "total_proxies": len(self.pool_a) + len(self.pool_b),
                "active_pool_health": len(self.pool_a if self.active_pool == "A" else self.pool_b) / self.config["batch_size"],
                "expired_ratio": len([p for p in (self.pool_a + self.pool_b) if p.is_expired]) / max(1, len(self.pool_a) + len(self.pool_b))
            }
        }
```

---

## ğŸ¯ å…³é”®ä¼˜åŒ–ç‚¹æ€»ç»“

### 1. æ‰¹é‡é™åˆ¶ä¿®æ­£ âœ…
- **ç”Ÿäº§ç¯å¢ƒ**: æœ€å¤§200ä¸ªä»£ç† (éµå¾ªæ¥å£é™åˆ¶)
- **å¼€å‘ç¯å¢ƒ**: æœ€å¤§20ä¸ªä»£ç† (ç”¨æˆ·æŒ‡å®šæµ‹è¯•é™åˆ¶)
- **ä¸¥æ ¼æ£€æŸ¥**: æ‰€æœ‰ä»£ç†è·å–éƒ½æ£€æŸ¥æ•°é‡ä¸Šé™

### 2. 7åˆ†é’Ÿè½®æ¢ç­–ç•¥ âœ…
- **è½®æ¢é—´éš”**: 7åˆ†é’Ÿ = 420ç§’
- **é‡å çª—å£**: 3åˆ†é’Ÿç¼“å†²æœŸ
- **é¢„çƒ­æœºåˆ¶**: 2åˆ†é’Ÿæå‰å‡†å¤‡

### 3. 10åˆ†é’ŸIPç”Ÿå‘½å‘¨æœŸ âœ…
- **è¿‡æœŸæ£€æŸ¥**: è‡ªåŠ¨æ¸…ç†10åˆ†é’Ÿä»¥ä¸Šä»£ç†
- **æ€§èƒ½ä¼˜åŒ–**: å¼‚æ­¥æ¸…ç†å’Œé¢„å–
- **ç»Ÿè®¡ç›‘æ§**: è¿‡æœŸç‡å’Œå¥åº·åº¦æŒ‡æ ‡

### 4. æ•°æ®åº“é›†æˆ âœ…
- **URLè¯»å–**: ç›´æ¥ä»æ•°æ®åº“è·å–ä»£ç†URL
- **è´¨é‡æ’åº**: æŒ‰è´¨é‡åˆ†æ•°å’Œä½¿ç”¨æ—¶é—´ä¼˜é€‰
- **è¿æ¥ç®¡ç†**: é€šè¿‡ä¾èµ–æ³¨å…¥ç®¡ç†æ•°æ®åº“è¿æ¥

---

## ğŸ“Š é…ç½®å¯¹æ¯”è¡¨

| é…ç½®é¡¹ | å¼€å‘ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ | è¯´æ˜ |
|--------|----------|----------|------|
| æ‰¹é‡å¤§å° | 20 | 200 | ç”¨æˆ·æŒ‡å®šé™åˆ¶ |
| æœ€å°é˜ˆå€¼ | 10 | 100 | è§¦å‘è¡¥å……çš„ä¸´ç•Œç‚¹ |
| é¢„å–ç¼“å†² | 5 | 80 | å¾…æœºæ± é¢„ç•™æ•°é‡ |
| è½®æ¢é—´éš” | 7åˆ†é’Ÿ | 7åˆ†é’Ÿ | å›ºå®šè½®æ¢å‘¨æœŸ |
| IPç”Ÿå‘½å‘¨æœŸ | 10åˆ†é’Ÿ | 10åˆ†é’Ÿ | å›ºå®šè¿‡æœŸæ—¶é—´ |

---

## ğŸš€ éƒ¨ç½²å’Œé…ç½®

### ç¯å¢ƒå˜é‡é…ç½®
```bash
# ä¿®æ­£ç‰ˆæœ¬çš„ç¯å¢ƒé…ç½®
PROXY_ENVIRONMENT=development  # æˆ– production
PROXY_BATCH_SIZE_DEV=20       # å¼€å‘ç¯å¢ƒæ‰¹é‡å¤§å°
PROXY_BATCH_SIZE_PROD=200     # ç”Ÿäº§ç¯å¢ƒæ‰¹é‡å¤§å° (æ¥å£ä¸Šé™)
ROTATION_INTERVAL_SECONDS=420  # 7åˆ†é’Ÿè½®æ¢
IP_LIFETIME_SECONDS=600       # 10åˆ†é’ŸIPç”Ÿå‘½å‘¨æœŸ
```

### ä½¿ç”¨ç¤ºä¾‹
```python
# åˆå§‹åŒ–ä¼˜åŒ–çš„A/Bæ± 
repo = OptimizedABPoolRepository(environment="development")
await repo.initialize(db_connection)

# è·å–ä»£ç† (è‡ªåŠ¨éµå¾ª20ä¸ªé™åˆ¶)
proxy_url = await repo.get_proxy("short")

# æ‰§è¡Œè½®æ¢ (7åˆ†é’Ÿé—´éš”)
success = await repo.perform_rotation()

# æŸ¥çœ‹ç»Ÿè®¡ (ç›‘æ§æ‰¹é‡é™åˆ¶éµå¾ªæƒ…å†µ)
stats = repo.get_statistics()
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 (æ‰¹é‡é™åˆ¶ä¿®æ­£ç‰ˆ)
**æ›´æ–°æ—¶é—´**: 2025-09-18
**éµå¾ªçº¦æŸ**: âœ… æœ€å¤§200ä¸ªä»£ç†ï¼Œæµ‹è¯•20ä¸ªï¼Œæ•°æ®åº“URLè¯»å–